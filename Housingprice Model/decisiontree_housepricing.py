# -*- coding: utf-8 -*-
"""DecisionTree_Housepricing.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1iwaHN7aNVDj4BIVKBeapP0WeeFONkadD
"""

import pandas as pd
from sklearn.model_selection import train_test_split, GridSearchCV
from sklearn.tree import DecisionTreeRegressor
from sklearn.preprocessing import StandardScaler
from sklearn.pipeline import Pipeline
from sklearn.preprocessing import OneHotEncoder
from sklearn.compose import ColumnTransformer
from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score
from datacleaninghousepricing import clean_data_Housepricing

def run_dectree_Housepricing():

    df = clean_data_Housepricing()

    y = df['SalePrice']
    X = df.drop('SalePrice', axis = 1)

    # Train/test split: 80% train, 20% test
    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

    # Define categorical and numerical feature sets
    cat_cols = ['Location', 'Basement available', 'Quality',
                'Air Conditioning available']

    num_cols = ['Property Area (ft²)', 'Living Area without Basement (ft²)',
                'Basement Area (ft²)', 'Beds', 'Kitchens', 'Baths',
                'Garage Capacity', 'Year Built',
                'last Renovation (year)']

    #Apply scaling to numerical and one-hot encoding to categorical features
    preprocessor = ColumnTransformer([
        ('cat', OneHotEncoder(handle_unknown='ignore'), cat_cols)
    ], remainder='passthrough')

    # Combine preprocessing and model into a single pipeline
    # defining the model and prepare the columns
    pipeline = Pipeline([
        ('preprocessor', preprocessor),
        ('dectree', DecisionTreeRegressor(random_state= 42, max_depth= 8,
                                          min_samples_split= 5, min_samples_leaf= 2))
    ])

    param_grid = {
        'dectree__max_depth' : [3, 6, 9, 12, 15, None],
        'dectree__min_samples_split' : [2, 5, 8, 12],
        'dectree__min_samples_leaf' : [1, 2, 5, 8, 12]
    }

    # Performs hyperparameter tuning by training the model with different
    # parameter values and selects the best parameter value based on cross-validation
    # performance
    grid_search = GridSearchCV(estimator= pipeline, param_grid= param_grid, cv= 5, n_jobs= -1, verbose= 1)
    grid_search.fit(X_train, y_train)

    # I print the optimal parameters for better reproduction
    print("Beste Parameter:", grid_search.best_params_)

    y_pred = grid_search.predict(X_test)

    # Evaluate model performance
    metrics = {
        # Average error in the correct unit.
        # Calculates how many dollars I am off on average
        "mean_squared_error": mean_squared_error(y_test, y_pred),
        # Error squared on average, is useful in business cases
        "mean_absolute_error": mean_absolute_error(y_test, y_pred),
        # How much variance the model can explain
        # Shows how well my model compares to the mean
        "r2_score": r2_score(y_test, y_pred)
    }

    return metrics